{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning \n",
    "Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani is considered a canonical text in the field of statistical/machine learning and is an absolutely fantastic way to move forward in your analytics career. [The text is free to download](http://www-bcf.usc.edu/~gareth/ISL/) and an [online course by the authors themselves](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about) is currently available in self-pace mode, meaning you can complete it any time. Make sure to **[REGISTER FOR THE STANDFORD COURSE!](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about)** The videos have also been [archived here on youtube](http://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/).\n",
    "\n",
    "# How will Houston Data Science cover the course?\n",
    "The Stanford online course covers the entire book in 9 weeks and with the R programming language. The pace that we cover the book is yet to be determined as there are many unknown variables such as interest from members, availability of a venue and general level of skills of those participating. That said, a meeting once per week to discuss the current chapter or previous chapter solutions is the target.\n",
    "\n",
    "\n",
    "# Python in place of R\n",
    "Although R is a fantastic programming language and is the language that all the ISLR labs are written in, the Python programming language, except for rare exceptions, contains analgous libraries that contain the same statistical functionality as those in R.\n",
    "\n",
    "# Notes, Exercises and Programming Assignments all in the Jupyter Notebok\n",
    "ISLR has both end of chapter problems and programming assignments. All chapter problems and programming assignments will be answered in the notebook.\n",
    "\n",
    "# Replicating Plots\n",
    "The plots in ISLR are created in R. Many of them will be replicated here in the notebook when they appear in the text\n",
    "\n",
    "# Book Data\n",
    "The data from the books was downloaded using R. All the datasets are found in either the MASS or ISLR packages. They are now in the data directory. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mAdvertising.csv\u001b[m\u001b[m* carseats.csv     khan_xtrain.csv  portfolio.csv\r\n",
      "Credit.csv       college.csv      khan_ytest.csv   smarket.csv\r\n",
      "auto.csv         default.csv      khan_ytrain.csv  usarrests.csv\r\n",
      "boston.csv       hitters.csv      nci60_data.csv   \u001b[31mwage.csv\u001b[m\u001b[m*\r\n",
      "caravan.csv      khan_xtest.csv   nci60_labs.csv   weekly.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISLR Videos\n",
    "[All Old Videos](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Unsupervised Learning\n",
    "Book has been about supervised learning until now. No response variable. Will try and find interesting things in explanatory variables. Chapter will focus on principal components analysis and clustering. No always an exact goal.\n",
    "\n",
    "## Principal Components Analysis\n",
    "Chapter 6 covered principal component regression where the original features were mapped to a smaller feature space that are then used as inputs into linear regression solved normally through least squares.\n",
    "\n",
    "PCA can be used to visualize high dimensional data in 2 or 3 dimensions. The first component is a weighted linear combination of all the original features where the sum of the squared weights equals 1. These weights are the loading factors. The loading factors of the first principal component maximize the weighted sum of the features for each observation.\n",
    "\n",
    "The second principal component is uncorrelated with the first which makes it orthogonal to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
